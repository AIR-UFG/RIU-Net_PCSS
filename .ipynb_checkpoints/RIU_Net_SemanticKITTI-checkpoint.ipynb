{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vjW00oucPm5G"
   },
   "source": [
    "# RIU-Net for SemanticKITTI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CpTYuNbmw4Rw"
   },
   "source": [
    "## Setup\n",
    "- In the case of not using google drive, just swap the paths with the path in you machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "3ba1BI7PPwcz"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import torch, os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image as PILImage\n",
    "from IPython.display import Image, display\n",
    "from google.colab.patches import cv2_imshow\n",
    "from torch.utils.data import Dataset\n",
    "from torch.nn import ReLU\n",
    "from torch.nn import Sequential\n",
    "from torch.nn import BatchNorm2d\n",
    "from torch.nn import ConvTranspose2d\n",
    "from torch.nn import Conv2d\n",
    "from torch.nn import MaxPool2d\n",
    "from torch.nn import Module\n",
    "from torch.nn import ModuleList\n",
    "from torchvision.transforms import CenterCrop\n",
    "from torch.nn import functional as F\n",
    "from tqdm import tqdm\n",
    "from imutils import paths\n",
    "from torch.optim import Adam\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms as tfms\n",
    "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive/', force_remount=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're using nbdev, by default Jupyter Lab initializes within the nbs folder, so it's a good practice to return to the root directory and save the path to avoid issues with paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/lucas/AIR/RIU-Net_PCSS'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#os.chdir(\"..\") comment this line if you're not using nbdev\n",
    "ROOT_PATH = os.getcwd()\n",
    "ROOT_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To work with the AIR libraries, it's necessary to clone the GitHub repositories, navigate to the directory, perform the installation and import, and then return to the root directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "libraries_path = ROOT_PATH+\"/libraries\"\n",
    "\n",
    "if not os.path.exists(libraries_path):\n",
    "    os.makedirs(libraries_path)\n",
    "\n",
    "os.chdir(libraries_path)\n",
    "\n",
    "#--depth 1 flag load only the last commit since the repositories are still under development.\n",
    "!git clone --depth 1 https://github.com/AIR-UFG/Cloud2DImageConverter.git\n",
    "!git clone --depth 1 https://github.com/AIR-UFG/SemanticKITTI_Tools.git\n",
    "\n",
    "os.chdir(libraries_path+\"/Cloud2DImageConverter\")\n",
    "!pip install -e '.[dev]'\n",
    "from Cloud2DImageConverter import api\n",
    "\n",
    "os.chdir(libraries_path+\"/SemanticKITTI_Tools\")\n",
    "!pip install -e '.[dev]'\n",
    "from SemanticKITTI_Tools import data\n",
    "\n",
    "os.chdir(ROOT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api.merge_images(origin_path=ROOT_PATH+\"/img_train/\", destiny_path=ROOT_PATH+\"/img_train/merged_imgs\")\n",
    "api.merge_images(origin_path=ROOT_PATH+\"/img_test/\", destiny_path=ROOT_PATH+\"/img_test/merged_imgs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "sWFY1O_KCJ59"
   },
   "outputs": [],
   "source": [
    "imgs_train_path = ROOT_PATH+\"/img_train/merged_imgs/\"\n",
    "masks_train_path = ROOT_PATH+\"/img_train/segmentation_mask/\"\n",
    "\n",
    "imgs_test_path = ROOT_PATH+\"/img_test/merged_imgs/\"\n",
    "masks_test_path = ROOT_PATH+\"/img_test/segmentation_mask/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-tyc-1KLKFBi",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Dataset validation and tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G-vWbUzIVBuF"
   },
   "outputs": [],
   "source": [
    "img_number = \"/000001.png\"\n",
    "img_path = imgs_train_path + img_number\n",
    "mask_path = masks_train_path + img_number\n",
    "\n",
    "print(img_path)\n",
    "print(mask_path)\n",
    "display(Image(filename=img_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TFfVV-oLVFvr"
   },
   "outputs": [],
   "source": [
    "display(Image(filename=mask_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fKtMM6mY_Ule"
   },
   "outputs": [],
   "source": [
    "img = PILImage.open(img_path)\n",
    "width, height = img.size\n",
    "width, height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hdq-oYKAwr3L"
   },
   "outputs": [],
   "source": [
    "img2 = PILImage.open(mask_path)\n",
    "np.unique(np.array(img2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gnZl3-oyrlUq"
   },
   "outputs": [],
   "source": [
    "# Checks if image.png == label.png\n",
    "files1 = sorted(os.listdir(imgs_train_path), reverse=True)\n",
    "files2 = sorted(os.listdir(masks_train_path), reverse=True)\n",
    "count = 0\n",
    "for file1, file2 in zip(files1, files2):\n",
    "  if file1 != file2:\n",
    "    print(f'Image {file1} does not have a corresponding mask, that being {file2}')\n",
    "  else:\n",
    "      count = count + 1\n",
    "if cont == len(files1) and len(files2):\n",
    "  print(f'Everything good to go, with {count} images')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0KH0kAg31nF1"
   },
   "source": [
    "## Configs\n",
    "- Tensor shape output at `SegmentationDataset` must be (B, C, H, W):\n",
    "  - B: BATCH_SIZE\n",
    "  - C: Image channels (2 for image_tensor and 1 or none for mask_tensor)\n",
    "  - H: Image height (64 for both)\n",
    "  - W: Image width (1024 for both)\n",
    "- Make sure `mask_tensor` values are not normalized in order to apply the binary mask later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "iyPcmpuitwkK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "VAL_SPLIT = 0.25\n",
    "\n",
    "# Device used for traing and evaluation\n",
    "DEVICE = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "\n",
    "PIN_MEMORY = True if DEVICE == \"cuda\" else False\n",
    "\n",
    "print(f\"Using {DEVICE} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "HmI1XQSnz6D3"
   },
   "outputs": [],
   "source": [
    "# HYPERPARAMETERS\n",
    "INIT_LR = 0.001\n",
    "NUM_EPOCHS = 10\n",
    "BATCH_SIZE = 8\n",
    "INPUT_IMAGE_WIDTH = 1024\n",
    "INPUT_IMAGE_HEIGHT = 64\n",
    "THRESHOLD = 0.5\n",
    "MODEL_PATH = \"unet_tgs_salt.pth\"\n",
    "PLOT_PATH = \"plot.png\"\n",
    "TEST_PATHS = \"test_paths.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EtjJ-9pGsiZ8",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "VnIxbiTYq_hj"
   },
   "outputs": [],
   "source": [
    "class Block(Module):\n",
    "\tdef __init__(self, inChannels, outChannels):\n",
    "\t\tsuper().__init__()\n",
    "\n",
    "\n",
    "\t\tself.conv1 = Sequential(Conv2d(inChannels, outChannels, 3, 1, 1), BatchNorm2d(outChannels, momentum = 0.99), ReLU())\n",
    "\t\tself.conv2 = Sequential(Conv2d(outChannels, outChannels, 3, 1, 1), BatchNorm2d(outChannels, 1e-05, 0.99), ReLU())\n",
    "\tdef forward(self, x):\n",
    "\n",
    "\t\toutputs1 = self.conv1(x)\n",
    "\t\toutputs2 = self.conv2(outputs1)\n",
    "\n",
    "\t\treturn outputs2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "nifhLgpzrWNB"
   },
   "outputs": [],
   "source": [
    "class Encoder(Module):\n",
    "\tdef __init__(self, channels=(2, 64, 128, 256, 512, 1024)):\n",
    "\t\tsuper().__init__()\n",
    "\n",
    "\n",
    "\t\tself.encBlocks = ModuleList([Block(channels[i], channels[i + 1])\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tfor i in range(len(channels) - 1)])\n",
    "\t\tself.pool = MaxPool2d(2)\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\n",
    "\t\tblockOutputs = []\n",
    "\t\t'''\n",
    "\t\tPass the inputs into the current encoder block;\n",
    "\t\tStore the block outputs and aply the max-pooling;\n",
    "\t\t'''\n",
    "\t\tfor block in self.encBlocks:\n",
    "\t\t\tx = block(x)\n",
    "\t\t\tblockOutputs.append(x)\n",
    "\t\t\tx = self.pool(x)\n",
    "\n",
    "\t\treturn blockOutputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "AMCS2FFdrmhx"
   },
   "outputs": [],
   "source": [
    "class Decoder(Module):\n",
    "\tdef __init__(self, channels=(1024, 512, 256, 128, 64)):\n",
    "\t\tsuper().__init__()\n",
    "\t\t'''\n",
    "\t\tUpsample and decoder blocks\n",
    "\t\t'''\n",
    "\n",
    "\t\tself.channels = channels\n",
    "\t\tself.upconvs = ModuleList([ConvTranspose2d(channels[i], channels[i + 1], 2, 2)\n",
    "\t\t\t \t\t\t\t\t\t\t\t\t\t\t\tfor i in range(len(channels) - 1)])\n",
    "\n",
    "\t\tself.dec_blocks = ModuleList([Block(channels[i], channels[i + 1])\n",
    "\t\t\t \t\t\t\t\t\t\t\t\t\t\t\t\t\tfor i in range(len(channels) - 1)])\n",
    "\n",
    "\tdef forward(self, x, encFeatures):\n",
    "\n",
    "\n",
    "\t\tfor i in range(len(self.channels) - 1):\n",
    "\n",
    "\t\t\tx = self.upconvs[i](x)\n",
    "\n",
    "\t\t\t'''\n",
    "\t\t\tCrop the current encoder block features;\n",
    "\t\t\tConcatenate with the current upsampled features;\n",
    "\t\t\tPass the output to the current decoder block\n",
    "\t\t\t'''\n",
    "\n",
    "\t\t\tencFeat = self.crop(encFeatures[i], x)\n",
    "\t\t\tx = torch.cat([x, encFeat], dim=1)\n",
    "\t\t\tx = self.dec_blocks[i](x)\n",
    "\n",
    "\t\treturn x\n",
    "\n",
    "\tdef crop(self, encFeatures, x):\n",
    "\t\t'''\n",
    "\t\tGet input dimensions;\n",
    "\t\tCrops encoder features in order to match input dimensions;\n",
    "\t\t'''\n",
    "\n",
    "\t\t(_, _, H, W) = x.shape\n",
    "\t\tencFeatures = CenterCrop([H, W])(encFeatures)\n",
    "\n",
    "\t\treturn encFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "RNeJeZWPr2g4"
   },
   "outputs": [],
   "source": [
    "class UNet(Module):\n",
    "  def __init__(self, encChannels=(2, 64, 128, 256, 512, 1024), decChannels=(1024, 512, 256, 128, 64), nbClasses=20):\n",
    "    super().__init__()\n",
    "\n",
    "    self.encoder = Encoder(encChannels)\n",
    "    self.decoder = Decoder(decChannels)\n",
    "\n",
    "    # Regression head\n",
    "    self.head = Conv2d(decChannels[-1], nbClasses, 1)\n",
    "\n",
    "  def forward(self, x):\n",
    "\n",
    "    encFeatures = self.encoder(x)\n",
    "\n",
    "    '''\n",
    "    Pass the encoder features through the decoder;\n",
    "    Make sure the dimensions are compatible for concatenation;\n",
    "    '''\n",
    "\n",
    "    decFeatures = self.decoder(encFeatures[::-1][0], encFeatures[::-1][1:])\n",
    "\n",
    "    # segmentation map\n",
    "    map = self.head(decFeatures)\n",
    "\n",
    "    return map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P5BGtE6HsbZ3"
   },
   "source": [
    "## Training\n",
    "\n",
    "- Image and mask paths for training\n",
    "- Dataset, DataLoader, Model, Opt and Loss instances\n",
    "- Binary mask is necessary to eliminate empty pixel loss from model bias.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "9bPEH64ispZx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] saving testing image paths...\n",
      "[INFO] testing image paths saved\n"
     ]
    }
   ],
   "source": [
    "# Store the test image path used on eval/test\n",
    "print(\"[INFO] saving testing image paths...\")\n",
    "f = open(TEST_PATHS, \"w\")\n",
    "f.write(\"\\n\".join(imgs_test_path))\n",
    "f.close()\n",
    "print(\"[INFO] testing image paths saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "XunfRb3bs2-R"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] found 4 examples in the training set...\n",
      "[INFO] found 4 examples in the test set...\n"
     ]
    }
   ],
   "source": [
    "transforms = tfms.Compose([tfms.ToTensor(), tfms.Normalize([0.2, 0.17], [0.16, 0.2])])\n",
    "\n",
    "# Datasets\n",
    "trainDS = data.SemanticDataset(image_path=imgs_train_path,\n",
    "                               mask_path=masks_train_path,\n",
    "                               transform=transforms)\n",
    "\n",
    "testDS = data.SemanticDataset(image_path=imgs_test_path,\n",
    "                              mask_path=masks_test_path,\n",
    "                              transform=transforms)\n",
    "\n",
    "\n",
    "print(f\"[INFO] found {len(trainDS)} examples in the training set...\")\n",
    "print(f\"[INFO] found {len(testDS)} examples in the test set...\")\n",
    "\n",
    "# DataLoaders\n",
    "trainLoader = DataLoader(trainDS,\n",
    "                         shuffle=True,\n",
    "                         batch_size=BATCH_SIZE, pin_memory=PIN_MEMORY,\n",
    "                         num_workers=os.cpu_count())\n",
    "\n",
    "testLoader = DataLoader(testDS, \n",
    "                        shuffle=False,\n",
    "                        batch_size=BATCH_SIZE, pin_memory=PIN_MEMORY,\n",
    "                        num_workers=os.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "9Y3v3B_YRlxl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train feature batch shape: torch.Size([4, 2, 64, 1024])\n",
      "Train labels batch shape: torch.Size([4, 64, 1024])\n",
      "Test feature batch shape: torch.Size([4, 2, 64, 1024])\n",
      "Test labels batch shape: torch.Size([4, 64, 1024])\n"
     ]
    }
   ],
   "source": [
    "# Checks Dataloader shape\n",
    "train_features, train_labels = next(iter(trainLoader))\n",
    "test_features, test_labels = next(iter(testLoader))\n",
    "\n",
    "print(f\"Train feature batch shape: {train_features.size()}\")\n",
    "print(f\"Train labels batch shape: {train_labels.size()}\")\n",
    "print(f\"Test feature batch shape: {test_features.size()}\")\n",
    "print(f\"Test labels batch shape: {test_labels.size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "Wv3Tgs2eu0hK"
   },
   "outputs": [],
   "source": [
    "unet = UNet().to(DEVICE)\n",
    "\n",
    "# reduction = 'none' in order to return the tensor with every pixel`s loss\n",
    "lossFunc = CrossEntropyLoss(reduction='none')\n",
    "\n",
    "# SGD optimizer can also be used instead of Adam\n",
    "opt = Adam(unet.parameters(), lr=INIT_LR)\n",
    "\n",
    "trainSteps = len(trainDS) // BATCH_SIZE\n",
    "testSteps = len(testDS) // BATCH_SIZE\n",
    "\n",
    "# Training history\n",
    "H = {\"train_loss\": [], \"test_loss\": []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MXXaUZ9twBSU"
   },
   "outputs": [],
   "source": [
    "# TRAINING\n",
    "print(\"[INFO] training the network...\")\n",
    "startTime = time.time()\n",
    "\n",
    "for e in tqdm(range(NUM_EPOCHS)):\n",
    "\tunet.train()\n",
    "\n",
    "\ttotalTrainLoss = 0\n",
    "\ttotalTestLoss = 0\n",
    "\tfor (i, (x, y)) in enumerate(trainLoader):\n",
    "\t\t(x, y) = (x.to(DEVICE), y.to(DEVICE))\n",
    "\n",
    "\t\tbin_mask_train = (y !=0).int()\n",
    "\t\tpred = unet(x)\n",
    "\t\tloss_train = lossFunc(pred, y)\n",
    "\n",
    "\t\tfiltered_loss_train = loss_train * bin_mask_train\n",
    "\t\tfiltered_loss_train = filtered_loss_train.mean()\n",
    "\n",
    "\t\topt.zero_grad()\n",
    "\t\tfiltered_loss_train.backward()\n",
    "\t\topt.step()\n",
    "\n",
    "\t\ttotalTrainLoss += filtered_loss_train\n",
    "\n",
    "\n",
    "\twith torch.no_grad():\n",
    "\t\tunet.eval()\n",
    "\t\tfor (x, y) in testLoader:\n",
    "\t\t\t(x, y) = (x.to(DEVICE), y.to(DEVICE))\n",
    "\n",
    "\t\t\tbin_mask_test = (y !=0).int()\n",
    "\t\t\tpred = unet(x)\n",
    "\t\t\tloss_test = lossFunc(pred, y)\n",
    "\n",
    "\t\t\tfiltered_loss_test = loss_test * bin_mask_test\n",
    "\t\t\tfiltered_loss_test = filtered_loss_test.mean()\n",
    "\n",
    "\t\t\ttotalTestLoss +=  filtered_loss_test\n",
    "\n",
    "\tavgTrainLoss = totalTrainLoss / trainSteps\n",
    "\tavgTestLoss = totalTestLoss / testSteps\n",
    "\n",
    "\t# Training history\n",
    "\tH[\"train_loss\"].append(avgTrainLoss.cpu().detach().numpy())\n",
    "\tH[\"test_loss\"].append(avgTestLoss.cpu().detach().numpy())\n",
    "\n",
    "\t# Model training validation info\n",
    "\tprint(\"[INFO] EPOCH: {}/{}\".format(e + 1, NUM_EPOCHS))\n",
    "\tprint(\"Train loss: {:.10f}, Test loss: {:.4f}\".format(avgTrainLoss, avgTestLoss))\n",
    "\n",
    "# Total trainning time\n",
    "endTime = time.time()\n",
    "print(\"[INFO] total time taken to train the model: {:.2f}s\".format(endTime - startTime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bKXX8VzjwW96"
   },
   "outputs": [],
   "source": [
    "# Training loss plot\n",
    "\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "plt.plot(H[\"train_loss\"], label=\"train_loss\")\n",
    "plt.plot(H[\"test_loss\"], label=\"test_loss\")\n",
    "plt.title(\"Training Loss on Dataset\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.savefig(PLOT_PATH)\n",
    "\n",
    "# Serialize model into disk\n",
    "torch.save(unet, MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cqHU6a34xbXm"
   },
   "source": [
    "##Prediction\n",
    "- Clone Cloud2DImageConverter repository for visualization.\n",
    "- Argmax is used for prediction.\n",
    "- Top image is ground truth.\n",
    "- Bottom image is the model prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yuMvCa2BxfUa"
   },
   "outputs": [],
   "source": [
    "def make_predictions(model, imagePath):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        image = PILImage.open(imagePath)\n",
    "        image = np.array(image).astype(np.float32)\n",
    "\n",
    "        # Ground truth path\n",
    "\n",
    "        filename = imagePath.split(os.path.sep)[-1]\n",
    "        groundTruthPath = os.path.join(MASK_TEST_DATASET_PATH, filename)\n",
    "\n",
    "\n",
    "        gtMask = PILImage.open(groundTruthPath)\n",
    "        gtMask = np.array(gtMask)\n",
    "\n",
    "        '''\n",
    "        Make channel axis to be the leading one;\n",
    "        Add batch dimension;\n",
    "        Create pytorch tensor;\n",
    "        Flash it to current device\n",
    "        '''\n",
    "        image = np.transpose(image, (2, 0, 1))\n",
    "        image = np.expand_dims(image, 0)\n",
    "        image = torch.from_numpy(image).to(DEVICE)\n",
    "\n",
    "        # Prediction\n",
    "\n",
    "        predMask = model(image).squeeze()\n",
    "\n",
    "        # shape must be (20,64,1024)\n",
    "        argmax = torch.argmax(predMask, dim=0)\n",
    "\n",
    "        # Cloud2DImageConverter api to convert index to the corresponding color\n",
    "\n",
    "        prediction = api.color_matrix(np.array(argmax.cpu()))\n",
    "        prediction = PILImage.fromarray(prediction)\n",
    "\n",
    "        gtMask = api.color_matrix(gtMask)\n",
    "        gtMask = PILImage.fromarray(gtMask)\n",
    "\n",
    "        # Visualization\n",
    "\n",
    "        display(gtMask)\n",
    "        display(prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2WJf26btxyAz"
   },
   "outputs": [],
   "source": [
    "# Sample selection\n",
    "\n",
    "print(\"[INFO] loading up test image paths...\")\n",
    "imagePaths = open(\"/content/test_paths.txt\").read().strip().split(\"\\n\")\n",
    "imagePaths = np.random.choice(imagePaths, size=10)\n",
    "\n",
    "print(\"[INFO] load up model...\")\n",
    "unet = torch.load(\"/content/unet_tgs_salt.pth\").to(DEVICE)\n",
    "\n",
    "for path in imagePaths:\n",
    "    make_predictions(unet, path)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
